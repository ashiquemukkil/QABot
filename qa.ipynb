{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Function\n",
    "This function takes in a string and performs a series of text cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    # Replacement of newline characters:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Stripping and reducing multiple spaces to single:\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "    # Removing backslashes:\n",
    "    cleaned_text = cleaned_text.replace(\"\\\\\", \"\")\n",
    "\n",
    "    # Replacing hash characters:\n",
    "    cleaned_text = cleaned_text.replace(\"#\", \" \")\n",
    "    cleaned_text = re.sub(r\"([^\\w\\s])\\1*\", r\"\\1\", cleaned_text)\n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Document:\n",
    "<p>\n",
    "The PDF document will be loaded using its file path.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from PDF using PyPDFLoader from LangChain\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfFileLoader:\n",
    "    def load_data(self, url):\n",
    "        \"\"\"Load data from a PDF file.\"\"\"\n",
    "        loader = PyPDFLoader(url)\n",
    "        output = []\n",
    "        pages = loader.load_and_split()\n",
    "        if not len(pages):\n",
    "            raise ValueError(\"No data found\")\n",
    "        for page in pages:\n",
    "            content = page.page_content\n",
    "            content = clean_string(content)\n",
    "            meta_data = page.metadata\n",
    "            meta_data[\"url\"] = url\n",
    "            output.append(\n",
    "                {\n",
    "                    \"content\": content,\n",
    "                    \"meta_data\": meta_data,\n",
    "                }\n",
    "            )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHUNKING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from typing import Optional,Callable\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from vectordb import ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_SPLITTER_CHUNK_PARAMS = {\n",
    "    \"chunk_size\": 200,\n",
    "    \"chunk_overlap\": 56,\n",
    "    \"length_function\": len,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkerConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: Optional[int] = 200,\n",
    "        chunk_overlap: Optional[int] = 56,\n",
    "        length_function: Optional[Callable[[str], int]] = len,\n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.length_function = length_function\n",
    "\n",
    "    def as_dict(self):\n",
    "        return vars(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfFileChunker:\n",
    "    def __init__(self,config: Optional[ChunkerConfig]=None):\n",
    "        \"\"\"Initialize the chunker.\"\"\"\n",
    "        if config is None:\n",
    "            config = TEXT_SPLITTER_CHUNK_PARAMS\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(**config)\n",
    "\n",
    "    def create_chunks(self, loader, src):\n",
    "        \"\"\"\n",
    "        Loads data and chunks it.\n",
    "\n",
    "        :param loader: The loader which's `load_data` method is used to create\n",
    "        the raw data.\n",
    "        :param src: The data to be handled by the loader. Can be a URL for\n",
    "        remote sources or local content for local loaders.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        ids = []\n",
    "        idMap = {}\n",
    "        datas = loader.load_data(src)\n",
    "        metadatas = []\n",
    "        for data in datas:\n",
    "            content = data[\"content\"]\n",
    "            meta_data = data[\"meta_data\"]\n",
    "            url = meta_data[\"url\"]\n",
    "\n",
    "            chunks = self.get_chunks(content)\n",
    "\n",
    "            for chunk in chunks:\n",
    "                chunk_id = hashlib.sha256((chunk + url).encode()).hexdigest()\n",
    "                if idMap.get(chunk_id) is None:\n",
    "                    idMap[chunk_id] = True\n",
    "                    ids.append(chunk_id)\n",
    "                    documents.append(chunk)\n",
    "                    metadatas.append(meta_data)\n",
    "        return {\n",
    "            \"documents\": documents,\n",
    "            \"ids\": ids,\n",
    "            \"metadatas\": metadatas,\n",
    "        }\n",
    "\n",
    "    def get_chunks(self, content):\n",
    "        \"\"\"\n",
    "        Returns chunks using text splitter instance.\n",
    "        \"\"\"\n",
    "        return self.text_splitter.split_text(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "import re\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitConfig:\n",
    "    def __init__(self, log_level=None, ef=None, db=None, host=None, port=None, id=None):\n",
    "        \"\"\"\n",
    "        :param log_level: Optional. (String) Debug level\n",
    "        ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'].\n",
    "        :param ef: Optional. Embedding function to use.\n",
    "        :param db: Optional. (Vector) database to use for embeddings.\n",
    "        :param id: Optional. ID of the app. Document metadata will have this id.\n",
    "        :param host: Optional. Hostname for the database server.\n",
    "        :param port: Optional. Port for the database server.\n",
    "        \"\"\"\n",
    "        self._setup_logging(log_level)\n",
    "        self.ef = ef\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.id = id\n",
    "        self._set_embedding_function()\n",
    "        self._set_db_to_default()\n",
    "        return\n",
    "\n",
    "    def _set_embedding_function(self, ef=None):\n",
    "        if  ef:\n",
    "            self.ef = ef\n",
    "        else:\n",
    "            self.ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "        return\n",
    "\n",
    "    def _set_embedding_function_to_default(self):\n",
    "        \"\"\"\n",
    "        Sets embedding function to default (`text-embedding-ada-002`).\n",
    "\n",
    "        :raises ValueError: If the template is not valid as template should contain\n",
    "        $context and $query\n",
    "        \"\"\"\n",
    "        if os.getenv(\"OPENAI_API_KEY\") is None and os.getenv(\"OPENAI_ORGANIZATION\") is None:\n",
    "            raise ValueError(\"OPENAI_API_KEY or OPENAI_ORGANIZATION environment variables not provided\")  # noqa:E501\n",
    "        self.ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            organization_id=os.getenv(\"OPENAI_ORGANIZATION\"),\n",
    "            model_name=\"text-embedding-ada-002\",\n",
    "        )\n",
    "        return\n",
    "    def as_dict(self):\n",
    "        return vars(self)\n",
    "        \n",
    "    def _set_db(self, db):\n",
    "        if db:\n",
    "            self.db = db\n",
    "        return\n",
    "\n",
    "    def _set_db_to_default(self):\n",
    "        \"\"\"\n",
    "        Sets database to default (`ChromaDb`).\n",
    "        \"\"\"\n",
    "        self.db = ChromaDB(ef=self.ef, host=self.host, port=self.port)\n",
    "\n",
    "    def _setup_logging(self, debug_level):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_PROMPT = \"\"\"\n",
    "  Use the following pieces of context to answer the query at the end.\n",
    "  If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "  $context\n",
    "\n",
    "  Query: $query\n",
    "\n",
    "  Helpful Answer:\n",
    "\"\"\"  # noqa:E501\n",
    "\n",
    "DEFAULT_PROMPT_WITH_HISTORY = \"\"\"\n",
    "  Use the following pieces of context to answer the query at the end.\n",
    "  If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "  I will provide you with our conversation history.\n",
    "\n",
    "  $context\n",
    "\n",
    "  History: $history\n",
    "\n",
    "  Query: $query\n",
    "\n",
    "  Helpful Answer:\n",
    "\"\"\"  # noqa:E501\n",
    "\n",
    "DOCS_SITE_DEFAULT_PROMPT = \"\"\"\n",
    "  Use the following pieces of context to answer the query at the end.\n",
    "  If you don't know the answer, just say that you don't know, don't try to make up an answer. Wherever possible, give complete code snippet. Dont make up any code snippet on your own.\n",
    "\n",
    "  $context\n",
    "\n",
    "  Query: $query\n",
    "\n",
    "  Helpful Answer:\n",
    "\"\"\"  # noqa:E501\n",
    "\n",
    "DEFAULT_PROMPT_TEMPLATE = Template(DEFAULT_PROMPT)\n",
    "DEFAULT_PROMPT_WITH_HISTORY_TEMPLATE = Template(DEFAULT_PROMPT_WITH_HISTORY)\n",
    "DOCS_SITE_PROMPT_TEMPLATE = Template(DOCS_SITE_DEFAULT_PROMPT)\n",
    "query_re = re.compile(r\"\\$\\{*query\\}*\")\n",
    "context_re = re.compile(r\"\\$\\{*context\\}*\")\n",
    "history_re = re.compile(r\"\\$\\{*history\\}*\")\n",
    "\n",
    "\n",
    "class QueryConfig:\n",
    "    \"\"\"\n",
    "    Config for the `query` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_documents=2,\n",
    "        template: Template = DEFAULT_PROMPT_WITH_HISTORY_TEMPLATE,\n",
    "        model=None,\n",
    "        temperature=None,\n",
    "        max_tokens=None,\n",
    "        top_p=None,\n",
    "        history=None,\n",
    "        stream: bool = False,\n",
    "    ):\n",
    "        if number_documents is None:\n",
    "            self.number_documents = 1\n",
    "        else:\n",
    "            self.number_documents = number_documents\n",
    "\n",
    "        if not history:\n",
    "            self.history = None\n",
    "        else:\n",
    "            if len(history) == 0:\n",
    "                self.history = None\n",
    "            else:\n",
    "                self.history = history\n",
    "\n",
    "        if template is None:\n",
    "            if self.history is None:\n",
    "                template = DEFAULT_PROMPT_TEMPLATE\n",
    "            else:\n",
    "                template = DEFAULT_PROMPT_WITH_HISTORY_TEMPLATE\n",
    "\n",
    "        self.temperature = temperature if temperature else 0\n",
    "        self.max_tokens = max_tokens if max_tokens else 1000\n",
    "        self.model = model if model else \"gpt-3.5-turbo-0613\"\n",
    "        self.top_p = top_p if top_p else 1\n",
    "\n",
    "        if self.validate_template(template):\n",
    "            self.template = template\n",
    "        else:\n",
    "            if self.history is None:\n",
    "                raise ValueError(\"`template` should have `query` and `context` keys\")\n",
    "            else:\n",
    "                raise ValueError(\"`template` should have `query`, `context` and `history` keys\")\n",
    "\n",
    "        if not isinstance(stream, bool):\n",
    "            raise ValueError(\"`stream` should be bool\")\n",
    "        self.stream = stream\n",
    "\n",
    "    def validate_template(self, template: Template):\n",
    "        if self.history is None:\n",
    "            return re.search(query_re, template.template) and re.search(context_re, template.template)\n",
    "        else:\n",
    "            return (\n",
    "                re.search(query_re, template.template)\n",
    "                and re.search(context_re, template.template)\n",
    "                and re.search(history_re, template.template)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddConfig:\n",
    "    \"\"\"\n",
    "    Config for the `add` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunker: Optional[ChunkerConfig] = None,\n",
    "        loader = None,\n",
    "    ):\n",
    "        self.loader = loader\n",
    "        self.chunker = chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHAT_PROMPT = \"\"\"\n",
    "  You are a chatbot having a conversation with a human. You are given chat\n",
    "  history and context.\n",
    "  You need to answer the query considering context, chat history and your knowledge base. If you don't know the answer or the answer is neither contained in the context nor in history, then simply say \"I don't know\".\n",
    "\n",
    "  $context\n",
    "\n",
    "  History: $history\n",
    "\n",
    "  Query: $query\n",
    "\n",
    "  Helpful Answer:\n",
    "\"\"\"  # noqa:E501\n",
    "\n",
    "DEFAULT_PROMPT_CHAT_TEMPLATE = Template(DEFAULT_CHAT_PROMPT)\n",
    "\n",
    "\n",
    "class ChatConfig(QueryConfig):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_documents=None,\n",
    "        template: Template = None,\n",
    "        model=None,\n",
    "        temperature=None,\n",
    "        max_tokens=None,\n",
    "        top_p=None,\n",
    "        stream: bool = False,\n",
    "    ):\n",
    "        if template is None:\n",
    "            template = DEFAULT_PROMPT_CHAT_TEMPLATE\n",
    "        super().__init__(\n",
    "            number_documents=number_documents,\n",
    "            template=template,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            history=[0],\n",
    "            stream=stream,\n",
    "        )\n",
    "\n",
    "    def set_history(self, history):\n",
    "        self.history = history\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4all_model = None\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ABS_PATH = os.getcwd()\n",
    "DB_DIR = os.path.join(ABS_PATH, \"db\")\n",
    "\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self, config: InitConfig):\n",
    "        \"\"\"\n",
    "        Initializes the EmbedChain instance, sets up a vector DB client and\n",
    "        creates a collection.\n",
    "\n",
    "        :param config: InitConfig instance to load as configuration.\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = config\n",
    "        self.db_client = self.config.db.client\n",
    "        self.collection = self.config.db.collection\n",
    "        self.user_asks = []\n",
    "        self.is_docs_site_instance = False\n",
    "        self.online = False\n",
    "\n",
    "    def add_local(self, data_type, content, metadata=None, config: AddConfig = None):\n",
    "        \"\"\"\n",
    "        Adds the data you supply to the vector db.\n",
    "        Loads the data, chunks it, create embedding for each chunk\n",
    "        and then stores the embedding to vector database.\n",
    "\n",
    "        :param data_type: The type of the data to add.\n",
    "        :param content: The local data. Refer to the `README` for formatting.\n",
    "        :param metadata: Optional. Metadata associated with the data source.\n",
    "        :param config: Optional. The `AddConfig` instance to use as\n",
    "        configuration options.\n",
    "        \"\"\"\n",
    "        if config is None:\n",
    "            config = AddConfig()\n",
    "\n",
    "        self.user_asks.append([data_type, content])\n",
    "        self.load_and_embed(\n",
    "            PdfFileLoader(),\n",
    "            PdfFileChunker(),\n",
    "            content,\n",
    "            metadata,\n",
    "        )\n",
    "\n",
    "    def load_and_embed(self, loader, chunker, src, metadata=None):\n",
    "        \"\"\"\n",
    "        Loads the data from the given URL, chunks it, and adds it to database.\n",
    "\n",
    "        :param loader: The loader to use to load the data.\n",
    "        :param chunker: The chunker to use to chunk the data.\n",
    "        :param src: The data to be handled by the loader. Can be a URL for\n",
    "        remote sources or local content for local loaders.\n",
    "        :param metadata: Optional. Metadata associated with the data source.\n",
    "        \"\"\"\n",
    "        embeddings_data = chunker.create_chunks(loader, src)\n",
    "        documents = embeddings_data[\"documents\"]\n",
    "        metadatas = embeddings_data[\"metadatas\"]\n",
    "        ids = embeddings_data[\"ids\"]\n",
    "        # get existing ids, and discard doc if any common id exist.\n",
    "        where = {\"app_id\": self.config.id} if self.config.id is not None else {}\n",
    "        # where={\"url\": src}\n",
    "        existing_docs = self.collection.get(\n",
    "            ids=ids,\n",
    "            where=where,  # optional filter\n",
    "        )\n",
    "        existing_ids = set(existing_docs[\"ids\"])\n",
    "\n",
    "        if len(existing_ids):\n",
    "            data_dict = {id: (doc, meta) for id, doc, meta in zip(ids, documents, metadatas)}\n",
    "            data_dict = {id: value for id, value in data_dict.items() if id not in existing_ids}\n",
    "\n",
    "            if not data_dict:\n",
    "                print(f\"All data from {src} already exists in the database.\")\n",
    "                return\n",
    "\n",
    "            ids = list(data_dict.keys())\n",
    "            documents, metadatas = zip(*data_dict.values())\n",
    "\n",
    "        # Add app id in metadatas so that they can be queried on later\n",
    "        if self.config.id is not None:\n",
    "            metadatas = [{**m, \"app_id\": self.config.id} for m in metadatas]\n",
    "\n",
    "        chunks_before_addition = self.count()\n",
    "\n",
    "        # Add metadata to each document\n",
    "        metadatas_with_metadata = [meta or metadata for meta in metadatas]\n",
    "\n",
    "        self.collection.add(documents=documents, metadatas=list(metadatas_with_metadata), ids=ids)\n",
    "        print((f\"Successfully saved {src}. New chunks count: \" f\"{self.count() - chunks_before_addition}\"))\n",
    "\n",
    "    def _format_result(self, results):\n",
    "        return [\n",
    "            (Document(page_content=result[0], metadata=result[1] or {}), result[2])\n",
    "            for result in zip(\n",
    "                results[\"documents\"][0],\n",
    "                results[\"metadatas\"][0],\n",
    "                results[\"distances\"][0],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def get_llm_model_answer(self, prompt):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def retrieve_from_database(self, input_query, config: QueryConfig):\n",
    "        \"\"\"\n",
    "        Queries the vector database based on the given input query.\n",
    "        Gets relevant doc based on the query\n",
    "\n",
    "        :param input_query: The query to use.\n",
    "        :param config: The query configuration.\n",
    "        :return: The content of the document that matched your query.\n",
    "        \"\"\"\n",
    "        where = {\"app_id\": self.config.id} if self.config.id is not None else {}  # optional filter\n",
    "        result = self.collection.query(\n",
    "            query_texts=[\n",
    "                input_query,\n",
    "            ],\n",
    "            n_results=config.number_documents,\n",
    "            where=where,\n",
    "        )\n",
    "        results_formatted = self._format_result(result)\n",
    "        contents = [result[0].page_content for result in results_formatted]\n",
    "        return contents\n",
    "\n",
    "    def _append_search_and_context(self, context, web_search_result):\n",
    "        return f\"{context}\\nWeb Search Result: {web_search_result}\"\n",
    "\n",
    "    def generate_prompt(self, input_query, contexts, config: QueryConfig, **kwargs):\n",
    "        \"\"\"\n",
    "        Generates a prompt based on the given query and context, ready to be\n",
    "        passed to an LLM\n",
    "\n",
    "        :param input_query: The query to use.\n",
    "        :param contexts: List of similar documents to the query used as context.\n",
    "        :param config: Optional. The `QueryConfig` instance to use as\n",
    "        configuration options.\n",
    "        :return: The prompt\n",
    "        \"\"\"\n",
    "        context_string = (\" | \").join(contexts)\n",
    "        web_search_result = kwargs.get(\"web_search_result\", \"\")\n",
    "        if web_search_result:\n",
    "            context_string = self._append_search_and_context(context_string, web_search_result)\n",
    "        if not config.history:\n",
    "            prompt = config.template.substitute(context=context_string, query=input_query)\n",
    "        else:\n",
    "            prompt = config.template.substitute(context=context_string, query=input_query, history=config.history)\n",
    "        return prompt\n",
    "\n",
    "    def get_answer_from_llm(self, prompt, config: ChatConfig):\n",
    "        \"\"\"\n",
    "        Gets an answer based on the given query and context by passing it\n",
    "        to an LLM.\n",
    "\n",
    "        :param query: The query to use.\n",
    "        :param context: Similar documents to the query used as context.\n",
    "        :return: The answer.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.get_llm_model_answer(prompt, config)\n",
    "\n",
    "    def access_search_and_get_results(self, input_query):\n",
    "        from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "        search = DuckDuckGoSearchRun()\n",
    "        return search.run(input_query)\n",
    "\n",
    "    def query(self, input_query, config: QueryConfig = None):\n",
    "        \"\"\"\n",
    "        Queries the vector database based on the given input query.\n",
    "        Gets relevant doc based on the query and then passes it to an\n",
    "        LLM as context to get the answer.\n",
    "\n",
    "        :param input_query: The query to use.\n",
    "        :param config: Optional. The `QueryConfig` instance to use as\n",
    "        configuration options.\n",
    "        :return: The answer to the query.\n",
    "        \"\"\"\n",
    "        if config is None:\n",
    "            config = QueryConfig()\n",
    "        if self.is_docs_site_instance:\n",
    "            config.template = DOCS_SITE_PROMPT_TEMPLATE\n",
    "            config.number_documents = 5\n",
    "        k = {}\n",
    "        if self.online:\n",
    "            k[\"web_search_result\"] = self.access_search_and_get_results(input_query)\n",
    "        contexts = self.retrieve_from_database(input_query, config)\n",
    "        prompt = self.generate_prompt(input_query, contexts, config, **k)\n",
    "\n",
    "        answer = self.get_answer_from_llm(prompt, config)\n",
    "\n",
    "        if isinstance(answer, str):\n",
    "            return answer\n",
    "        else:\n",
    "            return self._stream_query_response(answer)\n",
    "\n",
    "    def _stream_query_response(self, answer):\n",
    "        streamed_answer = \"\"\n",
    "        for chunk in answer:\n",
    "            streamed_answer = streamed_answer + chunk\n",
    "            yield chunk\n",
    "\n",
    "    def chat(self, input_query, config: ChatConfig = None):\n",
    "        \"\"\"\n",
    "        Queries the vector database on the given input query.\n",
    "        Gets relevant doc based on the query and then passes it to an\n",
    "        LLM as context to get the answer.\n",
    "\n",
    "        Maintains the whole conversation in memory.\n",
    "        :param input_query: The query to use.\n",
    "        :param config: Optional. The `ChatConfig` instance to use as\n",
    "        configuration options.\n",
    "        :return: The answer to the query.\n",
    "        \"\"\"\n",
    "        if config is None:\n",
    "            config = ChatConfig()\n",
    "        if self.is_docs_site_instance:\n",
    "            config.template = DOCS_SITE_PROMPT_TEMPLATE\n",
    "            config.number_documents = 5\n",
    "        k = {}\n",
    "        if self.online:\n",
    "            k[\"web_search_result\"] = self.access_search_and_get_results(input_query)\n",
    "        contexts = self.retrieve_from_database(input_query, config, **k)\n",
    "\n",
    "        global memory\n",
    "        chat_history = memory.load_memory_variables({})[\"history\"]\n",
    "        if chat_history:\n",
    "            config.set_history(chat_history)\n",
    "\n",
    "        prompt = self.generate_prompt(input_query, contexts, config, **k)\n",
    "        answer = self.get_answer_from_llm(prompt, config)\n",
    "\n",
    "        memory.chat_memory.add_user_message(input_query)\n",
    "\n",
    "        if isinstance(answer, str):\n",
    "            memory.chat_memory.add_ai_message(answer)\n",
    "            return answer\n",
    "        else:\n",
    "            # this is a streamed response and needs to be handled differently.\n",
    "            return self._stream_chat_response(answer)\n",
    "\n",
    "    def _stream_chat_response(self, answer):\n",
    "        streamed_answer = \"\"\n",
    "        for chunk in answer:\n",
    "            streamed_answer = streamed_answer + chunk\n",
    "            yield chunk\n",
    "        memory.chat_memory.add_ai_message(streamed_answer)\n",
    "\n",
    "    def count(self):\n",
    "        \"\"\"\n",
    "        Count the number of embeddings.\n",
    "\n",
    "        :return: The number of embeddings.\n",
    "        \"\"\"\n",
    "        return self.collection.count()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the database. Deletes all embeddings irreversibly.\n",
    "        `App` has to be reinitialized after using this method.\n",
    "        \"\"\"\n",
    "        self.db_client.reset()\n",
    "\n",
    "\n",
    "class OpenSourceApp(ChatBot):\n",
    "    \"\"\"\n",
    "    The OpenSource app.\n",
    "    Same as App, but uses an open source embedding model and LLM.\n",
    "\n",
    "    Has two function: add and query.\n",
    "\n",
    "    adds(data_type, url): adds the data from the given URL to the vector db.\n",
    "    query(query): finds answer to the given query using vector database and LLM.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: InitConfig = None):\n",
    "        \"\"\"\n",
    "        :param config: InitConfig instance to load as configuration. Optional.\n",
    "        `ef` defaults to open source.\n",
    "        \"\"\"\n",
    "        print(\"Loading open source embedding model. This may take some time...\")  # noqa:E501\n",
    "        if not config:\n",
    "            config = InitConfig()\n",
    "\n",
    "        if not config.ef:\n",
    "            config._set_embedding_function(\n",
    "                embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "            )\n",
    "\n",
    "        if not config.db:\n",
    "            config._set_db_to_default()\n",
    "\n",
    "        print(\"Successfully loaded open source embedding model.\")\n",
    "        super().__init__(config)\n",
    "\n",
    "    def get_llm_model_answer(self, prompt, config: ChatConfig):\n",
    "        global gpt4all_model\n",
    "        if gpt4all_model is None:\n",
    "            gpt4all_model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\n",
    "        response = gpt4all_model.generate(prompt=prompt, streaming=config.stream)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromadb_host = \"localhost\"\n",
    "chromadb_port = 8000\n",
    "\n",
    "config = InitConfig(host=chromadb_host, port=chromadb_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading open source embedding model. This may take some time...\n",
      "Successfully loaded open source embedding model.\n"
     ]
    }
   ],
   "source": [
    "qa_bot = OpenSourceApp(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data from /Users/muhammedashique/Downloads/manual.pdf already exists in the database.\n"
     ]
    }
   ],
   "source": [
    "qa_bot.add_local(\"pdf\", \"/Users/muhammedashique/Downloads/manual.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/muhammedashique/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[849]: Class GGMLMetalClass is implemented in both /Users/muhammedashique/QABot/.venv/lib/python3.8/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x283e70208) and /Users/muhammedashique/QABot/.venv/lib/python3.8/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x284374208). One of the two will be used. Which one is undefined.\n",
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from /Users/muhammedashique/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.06 MB\n",
      "llama_model_load_internal: mem required  = 2862.72 MB (+  682.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  650.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/muhammedashique/QABot/.venv/lib/python3.8/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x136bf5e80\n",
      "ggml_metal_init: loaded kernel_mul                            0x293debdd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x293dec0e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x293dec3f0\n",
      "ggml_metal_init: loaded kernel_silu                           0x293dec700\n",
      "ggml_metal_init: loaded kernel_relu                           0x293deca10\n",
      "ggml_metal_init: loaded kernel_gelu                           0x293decd20\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x293ded1c0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x293ded610\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x293deda80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x293dedef0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x293dee360\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_k                  0x293dee7d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_k                  0x293deec40\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_k                  0x293def0b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_k                  0x293def520\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_k                  0x293def990\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x293defe30\n",
      "ggml_metal_init: loaded kernel_norm                           0x293df02d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x293df0920\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x293df0df0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x293df12c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_k_f32               0x293df1790\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_k_f32               0x293df1e00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_k_f32               0x293df22d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_k_f32               0x293df27a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_k_f32               0x293df2c70\n",
      "ggml_metal_init: loaded kernel_rope                           0x293df3550\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x293df3c00\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x111f6dfb0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x111f6e750\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x111f6edd0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  1839.12 MB, ( 1839.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =   512.00 MB, ( 2351.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   652.00 MB, ( 3003.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   256.00 MB, ( 3259.52 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   256.00 MB, ( 3515.52 / 10922.67)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: max tensor size =    54.93 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' To protect your alloy wheels, you can use wheel guards or wheel covers. Wheel guards are usually made of a flexible material that fits over the wheel and provides protection from rocks, debris, and other hazards on the road. Wheel covers are also available in various sizes and shapes to fit different types of wheels. It is important to note that these protective measures should be taken regularly to ensure your alloy wheels remain safe and durable.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot.chat(\"How to protect the alloy wheel ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' AI: Yes, seat belts are essential for protecting passengers in case of a collision or other emergency situation. They provide a safety harness that holds the occupant in place, reducing the risk of injury. Additionally, wearing a seat belt can help prevent injuries by keeping the body properly positioned and aligned during a crash. It is important to always wear a seat belt when traveling in a vehicle.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot.chat(\"Seat belt usage is really necessary ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3da51e5d3455fb35e07f4855d99de36e2e9f69ea48c1864528e583158988bd64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
